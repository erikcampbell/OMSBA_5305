---
title: "working_hours_analysis_steamlined"
author: "Group 3"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(urca)
library(dynlm)
library(readxl)
library(forecast)
library(tidyverse)
library(tseries)
library(fpp2)
library(knitr)
library(stats)
library(ggpubr)
library(scales)
library(datplot)
library(kableExtra)
library(ggannotate) # this needs work
library(lmtest)
library(lubridate)
```

# Data Load 
```{r, echo=FALSE, warning=FALSE}
# Read in Data
prices <- read_excel("../data/HousePrices_CA.xls") 
working_hours = read_excel("../data/ch10_Weekly_Hours.xls") %>%
  mutate(year = year(observation_date),
         month = month(observation_date))

# Create covid and recession indicators associated with time.
working_hours1 <- working_hours %>%
  mutate(year = year(observation_date),
         month = month(observation_date),
         first_difference_index = working_hours$hours_worked_index-lag(working_hours$hours_worked_index)
         ) %>%
  select(-year, -month)

# Create a time series object
working_hours_ts = ts(working_hours$hours_worked_index, 
                      frequency = 12,
                      start = c(2006, 3))

# create First Difference of Hours Worked Index
working_hours_diff = diff(working_hours_ts)
```

# EDA
```{r, echo = FALSE}
plot_work_hours <- ggplot(working_hours1, aes(x = observation_date, 
                                             y = hours_worked_index)) +
  geom_line(size = 1) +
  stat_smooth(color = "blue", size =.4) +
  geom_rect(aes(xmin=as.POSIXct('2020-02-01'), xmax=as.POSIXct('2020-04-01'), ymin=-Inf, 
                ymax=Inf),linetype = 'dashed',color="light green", fill = 'light green', alpha=.01) +
  
  geom_rect(aes(xmin=as.POSIXct('2007-12-01'), xmax=as.POSIXct('2008-07-01'), ymin=-Inf, ymax=Inf), 
            linetype = 'dashed',color="light green", fill = 'light green', alpha=.01) +
  geom_hline(yintercept = 100, linetype = "dashed", color = "red") +

  labs(x = 'Date', y = 'Indexed Working Hours',
      title = 'Mar 2006 - MAR 2023 Agg. Weekly Work Hours Index Indexed(2007=100)', 
      subtitle =  'Recession(s) highlighed') +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")

plot_work_hours
```
The dotted-red line is base 2007 given a value of 100.  The black line is index value and a trend line in blue. The green periods are recessions. There is no doubt there an overall upward trend in the data and seasonality must be explored.

## Summary Stats Grouped Year - plot
```{r, echo = FALSE, warning= FALSE}
stats_year_base <- working_hours %>%
  select(year, hours_worked_index) %>%
  group_by(year) %>%
  summarise(year, avg = mean(hours_worked_index), median = median(hours_worked_index),
            stand_dev = sd(hours_worked_index)  
            ) %>%
  ggplot(aes(x = reorder(year, avg), y = avg)) +
  geom_point() +
  geom_errorbar(aes(ymin = avg - stand_dev, ymax = avg + stand_dev)) +
  coord_flip() +
  labs(title = 'MAR 2006 - Mar2023: Annual Mean & Variance', subtitle = '', 
       y = "Index Value with Base Year 2007 = 100", x = '') +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")

stats_year_base
```
This chart looks at the variance in index of hours worked by month from 2006-2023 and can shed some insight about cycles in working hours.

## Boxplot by Month to Evaluate Seasonality
```{r, echo=FALSE, warning=FALSE}
# Boxplot for cycle associations by month
ts_plot_season <- function(x = x) {
season <- cycle(x)
season.factor <- factor(season)
ggplot() + 
  geom_boxplot(mapping = aes(x = season.factor,
                             y = x)) +
  labs(x = "Month", y =  "Index Hours Worked",
       title = "Seasonality/Cycles: Agg Weekly Hours Indexed(2007 = 100) reported monthly") +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")
}
raw_season <- ts_plot_season(working_hours_ts) +
  ggtitle("Raw Data Seasonality/Cycles by Month All Years, Base 2007 = 100")
raw_season  

```
This boxplot is another method to look at potential seasonality by month from 2006-2023. The Q1 and Q4 are slightly higher than Q2 and Q3 as present in the box plot which is indicative of a cycle. First let's address the Trend and maybe the seaonsality with taking the first difference.

## Add in First Difference
```{r}
# create First Difference of Hours Worked Index
working_hours_diff = diff(working_hours_ts)

plot(working_hours_diff, main = "Working Hours Diff")
```
We can see that using the first difference removes the trend with the COVID outlier remaining. 

## Histogram data (Note: I removed the second diff as we did not model on it and it's not needed for this analysis)
```{r, echo = FALSE, warning=FALSE}
# Histogram - Basic 
hist_base <- gghistogram(working_hours_ts, fill = "blue", color = 'black', alpha = 0.50,
                    title ='Mar 2006 - MAR 2023', 
                    subtitle = 'Agg. Weekly Work Hours Index Indexed(2007=100)',
                    xlab = 'Index Value: Base (2007 = 100)', ylab = 'Frequency')  +
  geom_density(color = 'red' )

# Histogram - Basic for First Difference of Hours Worked Index
hist_diff <- gghistogram(working_hours_diff, fill = "blue", color = 'black', alpha = 0.50,
                    title ='First Difference: Mar 2006 - MAR 2023', 
                    subtitle = 'Agg. Weekly Work Hours Index Indexed(2007=100)',
                    xlab = 'Index Value: Base (2007 = 100)', ylab = 'Frequency')  +
                    xlim(-5,5) +
  geom_density(color = 'red' )

# Combination of Histograms with Raw Data, First Difference & Second Difference
hist_combo <- ggarrange(hist_base, hist_diff, nrow = 2, label.y  = 0.3,
                        font.label = list(size = 8, color = "black", face = "bold", family = NULL))
plot(hist_combo)
```
Here we can see the distribution differences between the raw data and the first difference data. The first difference data has a normal distribution indicating that the data is stationary and that we can model on it using the regression assumption necessary to draw inferences.

## Scatter plot by Year to Evaluate Seasonality
```{r, echo=FALSE}
## Seasonality check 
season_raw <- ggseasonplot(working_hours_ts) +
  ylab("Index Hours") +
  ggtitle("Raw:All Years Seasonal Plot") +
  theme_classic2()

# First Difference of Hours Worked Index and Seasonality Check
season_first_diff <- ggseasonplot(working_hours_diff) +
  ylab("Index Hours") +
  ggtitle("First Difference: Seasonal Plot All Years") +
  theme_classic2()

# Combined Plot of Raw vs. First Differenced Hours Worked Index
combo_diff_sec_diff_scatter <- ggarrange(season_raw, season_sec_diff,
          nrow = 2, common.legend = TRUE, legend = 'right', label.x = '')
combo_diff_sec_diff_scatter
```
There are 17 periods for each month with April being lowest aggregated and indexed to 2007 with a value 99.9, and December was highest aggregated and indexed value of 102.9 hours against base 2007. Seasonality plot for all years and months demonstrates the Great Recession and Covid period as illuminated in purple line not like any other of the other lines plotting index of hours worked over all twelve months from 2006-2023.

## Decompose data to inspect trend, seasonal and random events
```{r, echo=FALSE}
decomp_work_hours_ts <- decompose(working_hours_ts, "additive") # Yt = Tt + St + Rt
plot(decomp_work_hours_ts)

# First Differenced Hours Worked Index decomposition
decomp_work_hours_diff <- decompose(working_hours_diff, "additive") # Yt = Tt + St + Rt
plot(decomp_work_hours_diff)
```
The width and height of seasonal cycles over time is predictable and trend is fairly linear with the first difference of hours worked, or seasonal variation is relatively constant over time. Thus, decomposition additively (Yt = Tt + St + Rt) seems appropriate. Now that we have a solid EDA understanding of the data, let's check correlograms.

## Raw & First Difference Plot the autocorrelation and partial autocorrelations
```{r, echo= FALSE}
acf_12_diff <- ggAcf(working_hours_diff, lag.max = 12,  
             main = "First Difference ACF - Lag = 12")
pacf_12_diff <- ggPacf(working_hours_diff, lag.max = 12,
               main = "First Differnce PACF - Lag = 12")
acf_pacf_12_diff <- ggarrange(acf_12_diff, pacf_12_diff, nrow = 2) # lag = 12
acf_pacf_12_diff
```
With ACF and PACF spikes at the second lag suggests that a combined ARMA model of order 2 may be appropriate to consider. The MA(2), AR(2) and ARMA(2,2) models are still intuitive models to further research for modeling as the PACF is alternating through lag periods. 

# Augmented Dickey-Fuller test on Differenced Data to Check for Stationarity.
```{r, echo=FALSE, warning=FALSE}
lag_order <- ndiffs(working_hours_diff, test = "adf")  # Using ADF test for lag order selection

# Print the selected lag order
print(lag_order)
```
Double checked that the lag order should be 0 as the data is already differenced

```{r, echo=FALSE}
#DF test, k=5
df_diff_data_5 = ur.df(working_hours_diff, type = "trend", lags = 0)

summary(df_diff_data_5)
```
Dickey-Fuller suggests that we have stationary data now, as the p-value is below 0.01 and we can reject the null hypothesis that this data is not stationary. 

# Model Fitting and Analysis (2,1,2)
```{r}
# Fit an ARMA model with order of 2 for both terms with the first difference.
model_ARMA212 = arima(working_hours_ts, order = c(2, 1, 2))
summary(model_ARMA212) # RMSE = 1.275151

coeftest(model_ARMA202)

plot(model_ARMA212)
```

```{r}
# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts)
arma22_R2 = cor(fitted(model_ARMA212), working_hours_ts)^2  # 0.9618626
arma22_AR2 = 1 - (1-arma22_R2) * (n - 1) / (n - 2 - 1)
arma22_R2
arma22_AR2
```


```{r}
# Get the residuals from the ARIMA model
arma_residuals <- residuals(model_ARMA212)

y <- length(as.vector(working_hours_ts))

# Calculate the total sum of squares (TSS)
mean_y <- mean(as.vector(working_hours_ts))
tss <- sum((as.vector(working_hours_ts) - mean_y)^2)

# Calculate the residual sum of squares (RSS)
rss <- sum(arma_residuals^2)

# Calculate the number of predictors or parameters in the model (p)
p <- length(model_ARMA212$coef)

# Calculate the number of observations (n)
n <- length(y)

# Calculate R-squared
rsquared <- 1 - (rss / tss)

# Calculate the adjusted R-squared
adj_rsquared <- 1 - ((1 - rsquared) * (n - 1) / (n - p - 1))

# Print the calculated R-squared and adjusted R-squared values
print(rsquared)
print(adj_rsquared)
```

```{r}

# Residuals check ACF/PACF
checkresiduals(model_ARMA212)
```

```{r}
# Q-Test
r_2_box_test <- Box.test(model_ARMA212$residuals, lag = 5, type = "Ljung")
r_2_box_test # X-squared = 0.22148, df = 5, p-value = 0.9989
```

```{r}
# Forecast 6 months
fcast_ARMA212 = forecast(model_ARMA212, h=6)
autoplot(fcast_ARMA212, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_ARMA212)
```

So we have a reasonably good fit here with a ARMA(2,2) model the AIC is 689.72 which is relatively low, adjusted r-squared of 0.9611276, and phi parameters are both < 1, suggesting a good fit. As well RMSE is 1.2737 which suggests that this models predictions are roughly 1.27 units, or in our case hours, away from actual values. This is a good start.

The Ljung-Box statistic is used to see if all underlying population autocorrelations for the error may be zero. Our t-stat was 0.0015358 and p-value of 0.9968 given 5 lags, which is well above 0.05 -a significant threshold for non-zero autocorrelations that rejects null that residuals are independently distributed.

# AR(2)
```{r}
# Fit an AR model with order of 2 for both terms with the first difference.
model_AR210 = arima(working_hours_ts, order = c(2, 1, 0))
summary(model_AR210) # RMSE = 1.275151

coeftest(model_AR210)

plot(model_AR210)
```

```{r}
# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts)
ar2_R2 = cor(fitted(model_AR210), working_hours_ts)^2  # 0.9618626
ar2_AR2 = 1 - (1-ar2_R2) * (n - 1) / (n - 2 - 1)
ar2_R2
ar2_AR2
```


```{r}
# Get the residuals from the ARIMA model
ar_residuals <- residuals(model_AR210)

y <- length(as.vector(working_hours_ts))

# Calculate the total sum of squares (TSS)
mean_y <- mean(as.vector(working_hours_ts))
tss <- sum((as.vector(working_hours_ts) - mean_y)^2)

# Calculate the residual sum of squares (RSS)
rss <- sum(ar_residuals^2)

# Calculate the number of predictors or parameters in the model (p)
p <- length(model_AR210$coef)

# Calculate the number of observations (n)
n <- length(y)

# Calculate R-squared
rsquared <- 1 - (rss / tss)

# Calculate the adjusted R-squared
adj_rsquared <- 1 - ((1 - rsquared) * (n - 1) / (n - p - 1))

# Print the calculated R-squared and adjusted R-squared values
print(rsquared)
print(adj_rsquared)
```

```{r}
# Residuals check ACF/PACF
checkresiduals(model_AR210)
```

```{r}
# Q-Test
r_2_box_test <- Box.test(model_AR210$residuals, lag = 5, type = "Ljung")
r_2_box_test # X-squared = 0.22148, df = 5, p-value = 0.9989
```

```{r}
# Forecast 6 months
fcast_AR21 = forecast(model_AR210, h=6)
autoplot(fcast_AR21, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_AR21)
```
The AIC is 685.56, both phi parameters are < 1, our adjusted r-squared is 0.9614188 and RMSE is 1.2769. Our Q-test using Ljung was 0.24938500 and p-value looked good at 0.9692 with three lags. The AIC was lower than ARMA 2,1,2 and adjusted r-squared was marginally better.  

# MA(2)
```{r}
# Fit an MA model with order of 2 for both terms with the first difference.
model_MA012 = arima(working_hours_ts, order = c(0, 1, 2))
summary(model_MA012) # RMSE = 1.275151

coeftest(model_MA012)

plot(model_MA012)
```

```{r}
# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts)
ma2_R2 = cor(fitted(model_MA012), working_hours_ts)^2  # 0.9618626
ma2_AR2 = 1 - (1 - ma2_R2) * (n - 1) / (n - 2 - 1)
ma2_R2
ma2_AR2
```


```{r}
# Get the residuals from the ARIMA model
ma_residuals <- residuals(model_MA012)

y <- length(as.vector(working_hours_ts))

# Calculate the total sum of squares (TSS)
mean_y <- mean(as.vector(working_hours_ts))
tss <- sum((as.vector(working_hours_ts) - mean_y)^2)

# Calculate the residual sum of squares (RSS)
rss <- sum(ma_residuals^2)

# Calculate the number of predictors or parameters in the model (p)
p <- length(model_MA012$coef)

# Calculate the number of observations (n)
n <- length(y)

# Calculate R-squared
rsquared <- 1 - (rss / tss)

# Calculate the adjusted R-squared
adj_rsquared <- 1 - ((1 - rsquared) * (n - 1) / (n - p - 1))

# Print the calculated R-squared and adjusted R-squared values
print(rsquared)
print(adj_rsquared)
```

```{r}
# Residuals check ACF/PACF
checkresiduals(model_MA012)
```

```{r}
# Q-Test
r_2_box_test <- Box.test(model_MA012$residuals, lag = 5, type = "Ljung")
r_2_box_test # X-squared = 0.22148, df = 5, p-value = 0.9989
```

```{r}
# Forecast 6 months
fcast_MA21 = forecast(model_MA012, h=6)
autoplot(fcast_MA21, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_MA21)
```
The AIC is 685.25, much better than MA(1) and ARMA(2,2), but only marginally better than AR(2). The adjusted r-squared is 0.961485, just slighly lower than AR(2) and ARMA(2,2), and RMSE was 1.275489. The Ljung box test p-value looked good at 0.9886 at 3 lags.


# Step 2 DTC
## Forecasting Environment for Train/Test split: Estimation Sample and Prediciton Sample.
There are 204 observations and we'll be doing a 90%/10% split, thus our estimation sample with have 183 observations and our prediction sample will have 21 observations. We'll be applying a fixed scheme that will produce only one estimate and does not allow for parament updating. 

L(e) = ae^2 or Quadratic loss function.  h= 1

```{r, echo = FALSE}
es_work <- window(working_hours_diff,start=c(2006,4),end=c(2021,6)) 
length(es_work)# 183 observations
ps_work <- window(working_hours_diff, start=c(2021,7),end=c(2023,3))
length(ps_work) #21

# ARMA(2,1,2)
model_arma2112_estimate <- arima(es_work, order = c(2,1,2))
fcast1 <-forecast(model_arma2112_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                    # -0.05683105 -1.775835 1.662173 -2.685821 2.572158
e1 <- ps_work - fcast1$mean # 0.7568311
MSE1 <- mean(e1^2) # 0.5727932

# ARMA(2,1,0)
model_arma210_estimate <- arima(es_work, order = c(2,1,0))
fcast2 <-forecast(model_arma210_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                    # 0.2416706    -1.821441 2.304783 -2.913587 3.39692   
e2 <- ps_work - fcast2$mean # 0.4583294
MSE2 <- mean(e2^2) # 0.2100658

# ARMA(0,1,2)
model_arma012_estimate <- arima(es_work, order = c(0,1,2))
fcast4 <-forecast(model_arma012_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                  # 0.05086249     -1.694539 1.796264 -2.618499 2.720224 
e4 <- ps_work - fcast4$mean # 0.6491375
MSE4 <- mean(e4^2) # 0.4213795
```


```{r}
###########################################################################################################################

# Fixed Scheme - Professor
fcast1<-numeric(21) #generate a vector of 21 zeros as we have 132 total observations, using 112 as estimation sample 21= prediction sample.
model<-dynlm(working_hours_ts ~ stats::lag(working_hours_ts,-1)+ stats::lag(working_hours_ts,-2), start=c(2006,4), end=c(2021,6)) #fit AR(3), last 20 in t.s. stops
for (i in 1:21){ #start a for loop
  fcast1[i]<-coef(model)[1]+coef(model)[2]*working_hours_ts[182+i]+coef(model)[3]*working_hours_ts[181+i] #fill in forecasted values at the end of each iteration
} 

# Coefficients:
#                      (Intercept)  stats::lag(working_hours_ts, -1)  stats::lag(working_hours_ts, -2)  
#                          2.47647                           0.96502                           0.01104  


g0<-window(working_hours_ts, start=c(2021,7))
f1<-ts(fcast1, frequency = 12, start=c(2021,7))


fcast1<-numeric(21) #generate a vector of 20 zeros
ferror1<-numeric(21) #generate a vector of 20 zeros
loss1<-numeric(21) #generate a vector of 20 zeros

for (i in 1:21){ 
  fcast1[i]<-working_hours_ts[183+i]
  ferror1[i]<-working_hours_ts[183+i]- fcast2[i] #fill in forecasted values at the end of each iteration
  loss1[i] <-ferror1[i]^2
 } 
cbind(fcast2, ferror1, loss1)
MSE2 <- mean(loss1)

mpetest <- lm(ferror1 ~ 1)
summary(mpetest)


IETest <- lm(ferror1 ~ fcast2)
summary(IETest)


# FORECAST MA2
fcast2<-numeric(21) #generate a vector of 21 zeros as we have 132 total observations, using 112 as estimation sample 21= pred.
model2 <-dynlm(working_hours_ts ~ stats::lag(working_hours_ts,1)+ stats::lag(working_hours_ts,2), start=c(2006,4), end=c(2021,6)) #fit AR(3), last 20 in t.s. stops
for (i in 1:21){ #start a for loop
  fcast1[i]<-coef(model)[1]+coef(model)[2]*working_hours_ts[182+i]+coef(model)[3]*working_hours_ts[181+i] # forecasted values at the end of each iteration
} 

# 
# Coefficients:
#                     (Intercept)  stats::lag(working_hours_ts, 1)  stats::lag(working_hours_ts, 2)  
                        # 3.05034                          0.95877                          0.01055  


g0<-window(working_hours_ts, start=c(2021,7))
f2<-ts(fcast2, frequency = 12, start=c(2021,7))


fcast2<-numeric(21) #generate a vector of 20 zeros
ferror2<-numeric(21) #generate a vector of 20 zeros
loss2<-numeric(21) #generate a vector of 20 zeros

for (i in 1:21){ 
  fcast2[i]<-working_hours_ts[183+i]
  ferror2[i]<-working_hours_ts[183+i]- fcast2[i] #fill in forecasted values at the end of each iteration
  loss2[i] <-ferror2[i]^2
 } 
cbind(fcast2, ferror2, loss2)
MSE2 <- mean(loss2)

mpetest <- lm(ferror2 ~ 1)
summary(mpetest)


IETest <- lm(ferror2 ~ fcast2)
summary(IETest)



# FORECAST ARMA(2,2)
fcast2<-numeric(21) #generate a vector of 21 zeros as we have 132 total observations, using 112 as estimation sample 21= pred.
model2 <-dynlm(working_hours_ts ~ stats::lag(working_hours_ts,1)+ stats::lag(working_hours_ts,2), start=c(2006,4), end=c(2021,6)) #fit AR(3), last 20 in t.s. stops
for (i in 1:21){ #start a for loop
  fcast1[i]<-coef(model)[1]+coef(model)[2]*working_hours_ts[182+i]+coef(model)[3]*working_hours_ts[181+i] # forecast values at the end of each iteration
} 

arma_model <- dynlm(working_hours_ts ~ L(working_hours_ts, 1) + L(working_hours_ts, 2) + L(residuals(model), 1) + L(residuals(model), 2))   # we'll have to look into.


# 
# Coefficients:
#            (Intercept)  L(working_hours_ts, 1)  L(working_hours_ts, 2)  L(residuals(model), 1)  L(residuals(model), 2)  
#                -0.4676                  1.7679                 -0.7632                 -0.8300                 -0.1525  



fcast3<-numeric(21) #generate a vector of 20 zeros
ferror3<-numeric(21) #generate a vector of 20 zeros
loss3<-numeric(21) #generate a vector of 20 zeros

for (i in 1:21){ 
  fcast3[i]<-working_hours_ts[183+i]
  ferror3[i]<-working_hours_ts[183+i]- fcast2[i] #fill in forecasted values at the end of each iteration
  loss3[i] <-ferror1[i]^2
 } 
cbind(fcast3, ferror3, loss3)
MSE3 <- mean(loss3)

mpetest <- lm(ferror3 ~ 1)
summary(mpetest)


IETest <- lm(ferror3 ~ fcast3)
summary(IETest)


# Table 9.8  P246
# Optimal Linear Combinations


comb<-lm(g0~(fcast1)+(fcast2)+(fcast3))  # Work on weights w/ MSE's above.
summary(comb)

fcast4<-numeric(21) #generate a vector of 20 zeros
ferror4<-numeric(21) #generate a vector of 20 zeros
loss4<-numeric(21) #generate a vector of 20 zeros

fcast4 <- comb$fitted.values
ferror4 <- g0-fcast4
loss4 <-ferror4^2

MSE4 <- mean(loss4)

```