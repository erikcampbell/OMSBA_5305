---
title: "working_hours_analysis"
author: "Group 3"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

QUESTIONS:
Test our models on last 6 observations in data set as part of our forecast and evaluting fit, then provide a forecast?  We didn't do that to see realized values as part of forecast.




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(urca)
library(dynlm)
library(readxl)
library(forecast)
library(tidyverse)
library(tseries)
library(fpp2)
library(knitr)
library(stats)
library(ggpubr)
library(scales)
library(datplot)
library(kableExtra)
library(ggannotate) # this needs work
library(lmtest)
library(lubridate)
```

https://fred.stlouisfed.org/series/AWHAE
Our data is FRED creating an index (2007=100) of aggregate weekly hours reported on a monthly basis. The indexes of aggregate weekly hours are calculated by dividing the current month's aggregate hours by the average of the 12 monthly figures, for the base year (2007).

# EDA 
```{r, echo=FALSE, warning=FALSE}
# Read in Data
working_hours = read_excel("../data/ch10_Weekly_Hours.xls")

# Added as working_hours1 removed the year and month necessary for some functions to run.
working_hours <- working_hours %>%
  mutate(year = year(observation_date),
         month = month(observation_date))

# Create covid and recession indicators associated with time.
working_hours1 <- working_hours %>%
  mutate(year = year(observation_date),
         month = month(observation_date),
         first_difference_index = working_hours$hours_worked_index-lag(working_hours$hours_worked_index)
         ) %>%
  select(-year, -month)

# Create a time series object
working_hours_ts = ts(working_hours$hours_worked_index, 
                      frequency = 12,
                      start = c(2006, 3))

# create First Difference of Hours Worked Index
working_hours_diff = diff(working_hours_ts)

# Create a Second Difference of Hours Worked Index
working_hours_sec_diff <- diff(working_hours_diff)

plot(working_hours_diff, main = "Working Hours Diff")
plot(working_hours_sec_diff, main = "working Hours Second Diff")
```

## Plot Raw Data
```{r, echo = FALSE}
plot_work_hours <- ggplot(working_hours1, aes(x = observation_date, 
                                             y = hours_worked_index)) +
  geom_line(size = 1) +
  stat_smooth(color = "blue", size =.4) +
  geom_rect(aes(xmin=as.POSIXct('2020-02-01'), xmax=as.POSIXct('2020-04-01'), ymin=-Inf, 
                ymax=Inf),linetype = 'dashed',color="light green", fill = 'light green', alpha=.01) +
  
  geom_rect(aes(xmin=as.POSIXct('2007-12-01'), xmax=as.POSIXct('2008-07-01'), ymin=-Inf, ymax=Inf), 
            linetype = 'dashed',color="light green", fill = 'light green', alpha=.01) +
  geom_hline(yintercept = 100, linetype = "dashed", color = "red") +

  labs(x = 'Date', y = 'Indexed Working Hours',
      title = 'Mar 2006 - MAR 2023 Agg. Weekly Work Hours Index Indexed(2007=100)', 
      subtitle =  'Recession(s) highlighed') +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")

plot_work_hours
```
The dotted-red line is base 2007 given a value of 100.  The black line is index value and a trend line in blue. The green periods are recessions. There is no doubt there an overall upward trend in the data and seasonality must be explored.

## Density Plot
```{r, echo = FALSE}
# Density Plot for Raw Data
plot_work_hours1_density <- working_hours1 %>%
  ggplot(aes(x = hours_worked_index)) +
  geom_density(color="darkblue", fill="lightblue",alpha=0.7) +
  labs(x = "Index Hours 2007 =100", y = "", 
       title = "Raw Data Density", subtitle = "MAR2006-MAR2023") 

plot_work_hours1_density

# Density Plot for First Difference of Hours Worked Index
plot_work_hours_diff_density <- working_hours_diff %>%
  ggdensity(color="darkblue", fill="lightblue",alpha=0.7) +
  labs(x = "", y = "", 
       title = "First Difference Hours Worked Density", subtitle = "") +
  xlim(-2.5,2.5)

plot_work_hours_diff_density

# # Density Plot for Second Difference of Hours Worked Index
plot_work_hours_sec_diff_density <- working_hours_diff %>%
  ggdensity(color="darkblue", fill="lightblue",alpha=0.7) +
  labs(x = "", y = "", 
       title = "Second Difference Hours Worked Density", subtitle = "") +
  xlim(-2.5,2.5)

plot_work_hours_sec_diff_density

# Combine all Three Density Plots
ggarrange(plot_work_hours1_density, plot_work_hours_diff_density,
          plot_work_hours_sec_diff_density, nrow =1) 

```
The density of index values plotted raw is not very symmetrical, stationarity is a visual concern and transformations (i.e. taking first difference of index values and second index) do have an effect on the data.

## Histogram data
```{r, echo = FALSE, warning=FALSE}
# Histogram - Basic 
hist_base <- gghistogram(working_hours_ts, fill = "blue", color = 'black', alpha = 0.50,
                    title ='Mar 2006 - MAR 2023', 
                    subtitle = 'Agg. Weekly Work Hours Index Indexed(2007=100)',
                    xlab = 'Index Value: Base (2007 = 100)', ylab = 'Frequency')  +
  geom_density(color = 'red' )
hist_base

# Histogram - Basic for First Difference of Hours Worked Index
hist_diff <- gghistogram(working_hours_diff, fill = "blue", color = 'black', alpha = 0.50,
                    title ='First Difference: Mar 2006 - MAR 2023', 
                    subtitle = 'Agg. Weekly Work Hours Index Indexed(2007=100)',
                    xlab = 'Index Value: Base (2007 = 100)', ylab = 'Frequency')  +
                    xlim(-5,5) +
  geom_density(color = 'red' )
hist_diff

# Histogram - Basic for Second Difference of Hours Worked Index
hist_sec_diff <- gghistogram(working_hours_sec_diff, fill = "blue", color = 'black', alpha = 0.50,
                    title ='Second Difference: Mar 2006 - MAR 2023', 
                    subtitle = 'Agg. Weekly Work Hours Index Indexed(2007=100)',
                    xlab = 'Index Value: Base (2007 = 100)', ylab = 'Frequency')  +
                    xlim(-5,5) +
  geom_density(color = 'red' )
hist_sec_diff

# Combination of Histograms with Raw Data, First Difference & Second Difference
hist_combo <- ggarrange(hist_base, hist_diff, hist_sec_diff, nrow = 3, label.y  = 0.3,
                        font.label = list(size = 8, color = "black", face = "bold", family = NULL))
plot(hist_combo)
```

The histogram depicts similar story as density plots above.

## Summary Stats Grouped Year - numerical
```{r, echo=FALSE, warning=FALSE}
sum_stats_year <- working_hours %>%
  group_by(year) %>%
  summarize(count = n(),mean = round(mean(hours_worked_index), 2), median = median(hours_worked_index),
            mode(hours_worked_index), sd = round(sd(hours_worked_index), 2)
            )
sum_stats_year
# create a table for grouped yearly stats for rmd
kable(sum_stats_year,digits = 2, caption = "Summary Stats Grouped Year")
```

## Summary Stats Grouped Year - plot
```{r, echo = FALSE, warning= FALSE}
stats_year_base <- working_hours %>%
  select(year, hours_worked_index) %>%
  group_by(year) %>%
  summarise(year, avg = mean(hours_worked_index), median = median(hours_worked_index),
            stand_dev = sd(hours_worked_index)  
            ) %>%
  ggplot(aes(x = reorder(year, avg), y = avg)) +
  geom_point() +
  geom_errorbar(aes(ymin = avg - stand_dev, ymax = avg + stand_dev)) +
  coord_flip() +
  labs(title = 'MAR 2006 - Mar2023: Annual Mean & Variance', subtitle = '', 
       y = "Index Value with Base Year 2007 = 100", x = '') +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")

stats_year_base
```

This chart demonstrates the variance of hours worked in any given year.  More visual evidence of non-stationarity and possible cycles in raw data.

## Summary Stats Grouped Month - numerical
```{r, echo=FALSE, warning=FALSE}
sum_stats_month <- working_hours %>%
  mutate(month = (month.abb[month])) %>%
  group_by(month) %>%
  summarize(count = n(), mean = round(mean(hours_worked_index), 2), median = median(hours_worked_index), mode(hours_worked_index), sd = round(sd(hours_worked_index), 2)
            )
sum_stats_month

# create a table for grouped yearly stats for rmd
kable(sum_stats_month,digits = 2, caption = "Summary Stats Grouped Month")
```

## Summary Stats Grouped Month - Plot
```{r, echo=FALSE, warning= FALSE}
stats_month_base <- working_hours %>%
  select(year, month, hours_worked_index) %>%
  mutate(month_name = as.factor(month.abb[month])) %>%
  group_by(month_name) %>%
  summarize(month_name, avg = mean(hours_worked_index), median = median(hours_worked_index),
            stand_dev = sd(hours_worked_index)  
            ) %>%
  ggplot(aes(x = reorder(month_name, avg), y = avg)) +
  geom_point() +
  geom_errorbar(aes(ymin = avg - stand_dev, ymax = avg + stand_dev)) +
  coord_flip() +
  labs(title = 'MAR 2006 - Mar 2023 Variance by Month  Base 2007 = 100', subtitle = 'Min, Mean, Max', 
       y = "", x = '') +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")

stats_month_base
```

This chart looks at the variance in index of hours worked by month from 2006-2023 and can shed some insight about cycles in working hours.

## Boxplot by Month to Evaluate Seasonality
```{r, echo=FALSE, warning=FALSE}
# Boxplot for cycle associations by month
ts_plot_season <- function(x = x) {
season <- cycle(x)
season.factor <- factor(season)
ggplot() + 
  geom_boxplot(mapping = aes(x = season.factor,
                             y = x)) +
  labs(x = "Month", y =  "Index Hours Worked",
       title = "Seasonality/Cycles: Agg Weekly Hours Indexed(2007 = 100) reported monthly") +
  theme(axis.text.y=element_text(face="bold", color="black", size=10, angle=0),
      axis.ticks.y=element_blank(),
      axis.title = element_text(face = "bold", color="black", size=10),
      axis.text.x = element_text(face = "bold", color="black", size=10),
      panel.background =element_rect('White'),
      legend.position = "none")
}
raw_season <- ts_plot_season(working_hours_ts) +
  ggtitle("Raw Data Seasonality/Cycles by Month All Years, Base 2007 = 100")
raw_season  

# First Difference of Hours Worked Index and Seasonality Check
first_diff_season <- ts_plot_season(working_hours_diff) +
  ggtitle("First Diffrence Data Seasonality/Cycles by Month All Years, Base 2007 = 100")
first_diff_season

# Second Difference of Hours Worked Index and Seasonality Check
sec_diff_season <- ts_plot_season(working_hours_sec_diff) +
  ggtitle("Second Difference Data Seasonality/Cycles by Month All Years, Base 2007 = 100")
sec_diff_season
  
# Combine Raw Data, First Difference and Second Difference into One Plot
ggarrange(raw_season, first_diff_season, sec_diff_season, nrow =3)

# Combine Raw Data & First Difference
ggarrange(raw_season, first_diff_season, nrow =2)
```

This boxplot is another method to look at potential seasonality by month from 2006-2023. The Q1 and Q4 are slightly higher than Q2 and Q3 as present in the box plot which is indicative of a cycle.

## Scatter plot by Year to Evaluate Seasonality
```{r, echo=FALSE}
## Seasonality check 
season_raw <- ggseasonplot(working_hours_ts) +
  ylab("Index Hours") +
  ggtitle("Raw:All Years Seasonal Plot") +
  theme_classic2()
season_raw

# First Difference of Hours Worked Index and Seasonality Check
season_first_diff <- ggseasonplot(working_hours_diff) +
  ylab("Index Hours") +
  ggtitle("First Difference: Seasonal Plot All Years") +
  theme_classic2()
season_first_diff

# Second Difference of Hours Worked Index and Seasonality Check
season_sec_diff <- ggseasonplot(working_hours_sec_diff) +
  ylab("Index Hours") +
  ggtitle("2nd Difference: Seasonal Plot All Years") +
  theme_classic2()
season_sec_diff

# Combined Plot of Raw vs. First Differenced Hours Worked Index
combo_raw_diff_scatter <- ggarrange(season_raw, season_first_diff, nrow = 2, common.legend = TRUE, 
          legend = 'right', label.x = '')
combo_raw_diff_scatter

# Combined Plot of Raw vs. First Differenced Hours Worked Index
combo_diff_sec_diff_scatter <- ggarrange(season_raw, season_sec_diff,
          nrow = 2, common.legend = TRUE, legend = 'right', label.x = '')
combo_diff_sec_diff_scatter
```

There are 17 periods for each month with April being lowest aggregated and indexed to 2007 with a value 99.9, and December was highest aggregated and indexed value of 102.9 hours against base 2007. Seasonality plot for all years and months demonstrates the Great Recession and Covid period as illuminated in purple line not like any other of the other lines plotting index of hours worked over all twelve months from 2006-2023.

## Decompose data to inspect trend, seasonal and random events
```{r, echo=FALSE}
decomp_work_hours_ts <- decompose(working_hours_ts, "additive") # Yt = Tt + St + Rt
plot(decomp_work_hours_ts)
plot(decomp_work_hours_ts$trend, main = "Trend Data", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100")
plot(decomp_work_hours_ts$seasonal, main = "Seasonal Data", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100")
plot(decomp_work_hours_ts$random, main ="Error Term Over Time", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100" )
plot(decomp_work_hours_ts$figure, main = " Effects & Change for the 12 Months",
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100",
     xlab = "Month")

# First Differenced Hours Worked Index decomposition
decomp_work_hours_diff <- decompose(working_hours_diff, "additive") # Yt = Tt + St + Rt
plot(decomp_work_hours_diff)
plot(decomp_work_hours_diff$trend, main = "Differenced Data -Trend", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100")
plot(decomp_work_hours_diff$seasonal, main = "Differenced Data Seasonal Data", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100")
plot(decomp_work_hours_diff$random, main ="Differenced Data Error Term Over Time", 
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100" )
plot(decomp_work_hours_diff$figure, main = "Differenced Data Effects & Change for the 12 Months",
     ylab = "Changes in Agg Monthly Hours Worked- Indexed to 2007 = 100",
     xlab = "Month")
```

The width and height of seasonal cycles over time is predictable and trend is fairly linear with the first difference of hours worked, or seasonal variation is relatively constant over time. Thus, decomposition additively (Yt = Tt + St + Rt) seems appropriate. Now that we have a solid EDA undestanding of the data, let's check correlograms.

# Correlograms, lag = 36 - I don't want to use this as there no reason to replot the raw data and print ACF/PACF with 36 lags? 

#CDH- This was the original correlogram plot. Not a replot. Hence the just default 36 lags. Also you can just add lag as a parameter for whatever value you want. And it was done prior to the seasonality / stationary check before. We can remove all these that are prior the first diff change.
```{r, echo=FALSE}
# Plot the autocorrelation and partial autocorrelation functions
ggtsdisplay(working_hours_ts, lag=12) # lag = 36

```
Once again, our data is FRED creating an index (2007=100) of aggregate weekly hours reported on a monthly basis.  The indexes of aggregate weekly hours are calculated by dividing the current month's aggregate hours by the average of the 12 monthly figures, for the base year (2007). 

We see a gradual ACF decay which suggests the data has a auto-regressive component. Then looking at the PACF we can see that there is one sharp spike at the start. Preliminary thoughts are these two things suggest an AR process of order 1 or 2, MA process of order 1 and 2, and  and ARMA process are logical modeling process.

## Correlograms
```{r, echo=FALSE, warning=FALSE}
# create acf/pacf with L = 36
acf_36_plot <- ggAcf(working_hours_ts, lag.max = 36,  # this is base data
            main = "ACF of Agg Weekly Hours Indexed(2007 = 100) reported monthly")

pacf_36_plot <- ggPacf(working_hours_ts, lag.max = 36,
               main = "PACF Agg Weekly Hours Indexed(2007 = 100) reported monthly")
acf_pacf_36_plots <- ggarrange(acf_36_plot, pacf_36_plot, nrow = 2) # lag = 24
acf_pacf_36_plots

# create acf/pacf with L = 12
acf_12_plot <- ggAcf(working_hours_ts, lag.max = 12,  
             main = "ACF of Agg Weekly Hours Indexed(2007 = 100) reported monthly")
pacf_12_plot <- ggPacf(working_hours_ts, lag.max = 12,
               main = "PACF Agg Weekly Hours Indexed(2007 = 100) reported monthly")
acf_pacf_12_plot <- ggarrange(acf_12_plot, pacf_12_plot, nrow = 2) # lag = 12
acf_pacf_12_plot

# combine both acf/pacf for Lags 12 and Lags 36 into one visual
acf_pacf_comb <- ggarrange(acf_36_plot, acf_12_plot, pacf_36_plot,pacf_12_plot, nrow = 2, ncol = 2) # lag = 24
acf_pacf_comb

# numeric acf
acf_numeric <- acf(working_hours_diff, lag.max = 12, plot = FALSE)
acf_numeric  # 1.000 -0.022 -0.158

# numeric pacf
pacf_numeric <- pacf(working_hours_diff, lag.max =12, plot = FALSE)
pacf_numeric # -0.022 -0.158 -0.025 -0.044
```

Next, we'll use Dickey-Fuller to test for unit root and to evaluate if our H:0 non-stationary data is true or not. If our critical value is not less than tau and p-value > .05, we'll transform the data by taking the first difference of working hours index.

## Dickey-Fuller Check Root Check - I'm not sure if we need to look at more than 2 or so lags?????
```{r, echo=FALSE}
# Perform the ADF test
adf_result = ur.df(working_hours_ts, type = "trend", selectlags = "AIC")  # lag 1 is selected as evidenced by test below.
# Print the ADF test results
summary(adf_result)
```
The test statistic in this case is -2.6455. The critical values for the test statistics are also provided, which are -3.99, -3.43, and -3.13 for the 1%, 5%, and 10% significance levels respectively. The p-value of 0.05388 is greater than 0.05, which is close enough to support rejecting it as 0.05 is not a hard limit. It does make sense to calculate the first difference and recheck Dickey-Fuller. 

```{r, echo=FALSE}
df_check <- ur.df(working_hours_ts, type = 'trend', lags =1)
summary(df_check) 
```

## Comparison of First Difference vs. Raw Data
```{r, echo = FALSE}
par(mfrow = c(2,1), mex = 0.6, cex = 0.8)
plot(ts(working_hours$hours_worked_index), main = 'Index of avg Hours Worked , Base = 2007 at 100', xlab = 'Date', 
     ylab = 'Index Value ',
     col = 'red')

plot(working_hours_diff, main = 'First Differenced Data', xlab = 'Date', # call "growth rate" or c
     ylab = 'First Difference Index Value')
     lines(working_hours_diff, col = 'blue')
```

## Raw & First Difference Plot the autocorrelation and partial autocorrelations
```{r, echo= FALSE}
diff_autocor <- ggtsdisplay(working_hours_ts, main = "raw data")
first_diff_autocor <- ggtsdisplay(working_hours_diff, main = "First Differenced Data", lag.max = 12)
```

### First Difference Correlograms
```{r, echo=FALSE}
# create acf/pacf with L = 36
acf_36_diff <- ggAcf(working_hours_diff, lag.max = 36,  
            main = "First Difference ACF - Lag = 36")

pacf_36_diff <- ggPacf(working_hours_diff, lag.max = 36,
               main = "First Difference PACF - Lag = 36")
acf_pacf_36_diff <- ggarrange(acf_36_diff, pacf_36_diff, nrow = 2) # lag = 24
acf_pacf_36_diff

# create acf/pacf with L = 12
acf_12_diff <- ggAcf(working_hours_diff, lag.max = 12,  
             main = "First Difference ACF - Lag = 12")
pacf_12_diff <- ggPacf(working_hours_diff, lag.max = 12,
               main = "First Differnce PACF - Lag = 12")
acf_pacf_12_diff <- ggarrange(acf_12_diff, pacf_12_diff, nrow = 2) # lag = 12
acf_pacf_12_diff

# combine both acf/pacf for Lags 12 and Lags 36 into one visual
acf_pacf_comb_diff <- ggarrange(acf_36_diff, acf_12_diff, pacf_36_diff,pacf_12_diff, nrow = 2, ncol = 2) # lag = 24
acf_pacf_comb_diff

# numeric acf
acf_numeric <- acf(working_hours_diff, lag.max = 12, plot = FALSE)
acf_numeric

# numeric pacf
pacf_numeric <- pacf(working_hours_diff, lag.max =12, plot = FALSE)
pacf_numeric
```

We can see that adding in the first difference makes us much more trend-stationary and it can be observed that the 2008 Great Recession and Covid outlier impacts are mitigated. With ACF and PACF spikes at the second lag suggests that a combined ARMA model of order 2 may be appropriate to consider. The MA(1), MA(2) and AR(2) models are still intuitive models to further research for modeling as the PACF is alternating through lag periods. 

# Augmented Dickey-Fuller test on Differenced Data to Check for Stationarity.
```{r, echo=FALSE, warning=FALSE}
# Perform the ADF test on the differenced data
adf_5 <- adf.test(working_hours_diff) # k = 5 in test   
adf_5 # -6.5349, lag 5 p-value = .01, Test Stat = -6.5349  Critical Values =tau3 -3.99 -3.43 -3.13
adf_2 <-adf.test(working_hours_diff, k = 2) #
adf_2  #p-value = 0.01 Test Stat = -9.3099  Critical Values = tau3 -3.99 -3.43 -3.13
```

## Alternative DF test (i.e.library(URCA)) that presents critical values
```{r, echo=FALSE}
#DF test, k=5
df_diff_data_5 = ur.df(working_hours_diff, type = "trend", lags = 5)
summary(df_diff_data_5)

# DF test, k=2
df_diff_data_2 = ur.df(working_hours_diff, type = "trend", lags = 2)
summary(df_diff_data_2)
```

Dickey-Fuller suggests that we have stationary data now, as the p-value of 0.01 is below 0.05 and we can reject the null hypothesis that this data is not stationary. 

# Fit model ARMA(2,1,2) https://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima
```{r, echo=FALSE}
# Fit an ARMA model with order of 2 for both terms.
model_ARMA202 = arima(working_hours_ts, order = c(2, 1, 2))
summary(model_ARMA202) # RMSE = 1.275151
plot(model_ARMA202)
coeftest(model_ARMA202)

# RE-DOING above working with ARIMA not doing the difference (i.e. (order = 2,1,2)). Note: Look at r^2 and AR^2 values are a joke using Arima(2,0,2) with working_hours_diff t.s. - See below.
model_ARMA202_diff = arima(working_hours_diff, order = c(2, 0, 2))
summary(model_ARMA202_diff) # RMSE = 1.273783
plot(model_ARMA202_diff)
coeftest(model_ARMA202_diff)

# Check r-squared comparing model_ARMA202_diff and manually calculating r-squared & adj. r-squared.
se <- sum((fitted(model_ARMA202_diff) - working_hours_diff)^2) # 330.9947 - this is in working_hours_difference units
ssr <- sum((fitted(model_ARMA202_diff) - mean(working_hours_diff))^2) # 9.605149
sst <- ssr + se # 340.5999
n = length(working_hours_diff) # 204
R2_model_ARMA202_diff <- (1 - (ssr/sst)) # R-squared = 0.9999955
AR2_model_ARMA202_diff <- 1-((1-R2_model_ARMA202_diff)*(n-1)/(n-4-1)) # 0.9712325

# R-Squared test methodology   # https://www.statology.org/sst-ssr-sse-in-r/
# Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]
# n = Number of points
# K = # of variables excluding constant
# Manually Calculating R2 and AR2 with formula
sse <- sum((fitted(model_ARMA202) - working_hours_diff)^2) # 2141297 - Note using difference working hours
ssr <- sum((fitted(model_ARMA202) - mean(working_hours_diff))^2) # 2150340
sst <- ssr + sse # 4291637

n = length(working_hours_diff) # 204
R2_model_ARMA202 <- (1 - (ssr/sst)) # R-squared = 0.4989465
AR2_model_ARMA202 <- 1-((1-R2_model_ARMA202)*(n-1)/(n-4-1))  # Adj.R-squared = 0.488875

# R-Squared from Week 6-Chapter 8 practice problem set
R2=cor(fitted(model_ARMA202),working_hours_ts)^2 # 0.9618898
n = length(working_hours_ts)
RA2 = RA2 = 1-(1-R2)*(n-1)/(n-4-1) # 0.9611276

# AIC/BIC
AIC(model_ARMA202) # 689.14
BIC(model_ARMA202) # 705.734

# Residuals check ACF/PACF
checkresiduals(model_ARMA202)
density_arma202 <- density(model_ARMA202$residuals)
plot(density_arma202, xlim = c(-5, 5), main = "ARMA (2,1,2) Residuals")

# Plot residuals in ACF/PACF
arma202_acf_residuals <- acf(model_ARMA202$residuals, main = "ACF Residuals")
arma202_pacf_residuals <- acf(model_ARMA202$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(arma202_acf_residuals, main = "ARMA (2,1,2) ACF Residuals")
plot(arma202_pacf_residuals, main = "ARMA (2,1,2) PACF Residuals")
qqnorm(residuals(model_ARMA202), main = 'ARMA(2,1,0) Residuals')
qqline(model_ARMA202$residuals, col = 'red')

# Q-Test
QTest<-numeric(5) # of lags - Why?  I think this is different from video?
for (i in 1:5){ 
  qt <- Box.test(model_ARMA202$residuals, lag=i, type="Ljung") # Copy this for later!!!!  Lag = 2 "not statistically significant?"
  QTest[i]=qt$statistic
} 

QTest # 0.01620484 0.05323215
plot(QTest)

ar_2_box_test <- Box.test(model_ARMA202$residuals, lag = 5, type = "Ljung")
ar_2_box_test # X-squared = 0.22148, df = 5, p-value = 0.9989

# Forecast 6 months
fcast_arma202 = forecast(model_ARMA202, h=6)
autoplot(fcast_arma202,include = 12, ylab = "Working hours - Base 2007 with 100")
summary(fcast_arma202)
```

So we have a reasonably good fit here with a ARMA(2,2) model the AIC is 689.72 which is relatively low, adjusted r-squared of 0.9611276, and phi parameters are both < 1, suggesting a good fit. As well RMSE is 1.2737 which suggests that this models predictions are roughly 1.27 units, or in our case hours, away from actual values. This is a good start.

The Ljung-Box statistic is used to see if all underlying population autocorrelations for the error may be zero. Our t-stat was 0.0015358 and p-value of 0.9968 given 5 lags, which is well above 0.05 -a significant threshold for non-zero autocorrelations that rejects null that residuals are independently distributed.

# Fit model ARMA(2,1,0)
```{r, echo = FALSE}
model_ar2 <- arima(working_hours_ts, order = c(2,1,0))
summary(model_ar2) # 1.27648  # Note: T-Ratio = Coefficient/Std. Error
plot(model_ar2)  
coeftest(model_ar2)

# R-Squared test methodology   # https://www.statology.org/sst-ssr-sse-in-r/
# Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]
# n = Number of points
# K = # of variables excluding constant

sse <- sum((fitted(model_ar2) - working_hours_diff)^2) # 2141658   / 334.0275- Note using difference working hours
ssr <- sum((fitted(model_ar2) - mean(working_hours_diff))^2) # 2150699  / 8471.919 
sst <- ssr + sse # 4292357   / 8805.946

n = length(working_hours_diff) # 204 / 205
R2_model_ar2 <- (1 - (ssr/sst)) # R-squared = 0.4989468    / 0.03793204
AR2_model_ar2 <- 1-((1-R2_model_ar2 )*(n-1)/(n-2-1))  # Adj.R-squared = 0.4964664  /  0.02840661

# R-Squared from Week 6-Chapter 8 practice problem set
R2=cor(fitted(model_ar2),working_hours_ts)^2 # 0.9617971
n = length(working_hours_ts) # 205
RA2 = RA2 = 1-(1-R2)*(n-1)/(n-2-1) # 0.9614188

# AIC/BIC
AIC(model_ar2) # 685.56
BIC(model_ar2) # 695.5156

# Residuals check ACF/PACF
checkresiduals(model_ar2)
density_ar2 <- density(model_ar2$residuals)
plot(density_ar2, xlim = c(-5, 5), main = "ARMA (2,1,0) Residuals")

# Plot residuals in ACF/PACF
ar2_acf_residuals <- acf(model_ar2$residuals, main = "ACF Residuals")
ar2_pacf_residuals <- acf(model_ar2$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(ar2_acf_residuals, main = "ACF Residuals")
plot(ar2_pacf_residuals, main = "PACF Residuals")

qqnorm(residuals(model_ar2), main = 'ARMA(2,1,0) Residuals')
qqline(model_ar2$residuals, col = 'red')

# Q-Test
QTest<-numeric(3) # of lags - Why?  I think this is different from video?
for (i in 1:3){ 
  qt <- Box.test(model_ar2$residuals, lag=i, type="Ljung") # Copy this for later!!!!  Lag = 2 "not statistically significant?"
  QTest[i]=qt$statistic
} 

QTest # 0.24938500
plot(QTest)

ar_2_box_test <- Box.test(model_ar2$residuals, lag = 3, type = "Ljung")
ar_2_box_test # X-squared = 0.24939, df = 3, p-value = 0.9692

# Forecast 6 months
fcast_ar2 = forecast(model_ar2, h=6)
autoplot(fcast_ar2,include = 12, ylab = "Working hours - Base 2007 with 100")
summary(fcast_ar2)
```

The AIC is 685.56, both phi parameters are < 1, our adjusted r-squared is 0.9614188 and RMSE is 1.2769. Our Q-test using Ljung was 0.24938500 and p-value looked good at 0.9692 with three lags. The AIC was lower than ARMA 2,1,2 and adjusted r-squared was marginally better.  

# Fit Model MA(1) ARMA(0,1,1)
```{r, echo = FALSE}
model_ma1 <- arima(working_hours_ts, order = c(0,1,1))
summary(model_ma1) # 1.291698 
plot(model_ma1)
coeftest(model_ma1)

sse <- sum((fitted(model_ma1) - working_hours_diff)^2) # 2142246 - Note using difference working hours
ssr <- sum((fitted(model_ma1) - mean(working_hours_diff))^2) # 2151271
sst <- ssr + sse # 4293517

n = length(working_hours_diff) # 204
R2_model_ma1 <- (1 - (ssr/sst)) # R-squared = 0.498949
AR2_model_ma1 <- 1-((1-R2_model_ma1)*(n-1)/(n-1-1))  # Adj.R-squared =0.4964686

# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts)
ma1_R2 = cor(fitted(model_ma1), working_hours_ts)^2  # 0.960895
ma1_AR2 = 1 - (1-ma1_R2)*(n-1)/(n-1-1) #  0.9607024

AIC(model_ma1) # 688.3503
BIC(model_ma1) # 694.9865  

# Residuals check ACF/PACF
checkresiduals(model_ma1)
density_ma1 <- density(model_ma1$residuals)
plot(density_ma1, main = 'ARMA(0,1,1 Residuals Density') # need to change xlim

ma1_acf_residuals <- acf(model_ma1$residuals, main = "ACF Residuals")
ma1_pacf_residuals <- acf(model_ma1$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(ma1_acf_residuals, main = "ACF Residuals")
plot(ma1_pacf_residuals, main = "PACF Residuals")
# We have a spike at lag 2 (-0.158) that is significant and indicates more than ARMA(0,1,1) process.

qqnorm(residuals(model_ma1), main = 'ARMA(0,1,1 Residuals')
qqline(model_ma1$residuals, col = 'red')

# Q-Test
ma1_box_test <- Box.test(model_ma1$residuals, lag = 2, type = "Ljung")
ma1_box_test # X-squared = 5.246, df = 2, p-value = 0.072587  - over .05, 
            # but worrisome as close to serial correlation up to 2 lags

# Forecast 6 months
fcast_ma1 = forecast(model_ma1, h=6)
autoplot(fcast_ma1, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_ma1) 
```

The AIC is 688.3503 and adjusted r-squared is 0.9607024 and RMSE is 1.291698. The AIC was higher than both ARMA 2,1,2 and ARMA 2,1,0 and adjusted r-squared was lower. The p-value of Ljung was 0.072587 with 2 lags, not much above 0.05- a threshold for non-zero autocorrelations that rejects null that residuals are independently distributed. 
 
# Fit Model MA(2) ARMA(0,1,2)
```{r, echo=FALSE}
model_ma2 <- arima(working_hours_ts, order = c(0,1,2))
summary(model_ma2) # RMSE = 1.275489
plot(model_ma2)
coeftest(model_ma2)

sse <- sum((fitted(model_ma2) - working_hours_diff)^2) # 2141435 - Note using difference working hours
ssr <- sum((fitted(model_ma2) - mean(working_hours_diff))^2) # 2150477
sst <- ssr + sse # 4291912

n = length(working_hours_diff) # 204
R2_model_ma2 <- (1 - (ssr/sst)) # 0.4989466
AR2_model_ma2 <- 1-((1-R2_model_ma2)*(n-1)/(n-2-1))  # 0.493961

# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts)
ma2_R2 = cor(fitted(model_ma2), working_hours_ts)^2  # 0.9618626
ma2_AR2 = 1 - (1-ma2_R2)*(n-1)/(n-2-1) #  0.961485

AIC(model_ma2) # 685.25
BIC(model_ma2) # 695.2043 

# Residuals check ACF/PACF
checkresiduals(model_ma2)
density_ma2 <- density(model_ma2$residuals)
plot(density_ma2, main = 'ARMA(0,1,2 Residuals Density') # need to change xlim

ma2_acf_residuals <- acf(model_ma2$residuals, main = "ACf Residuals")
ma2_pacf_residuals <- acf(model_ma2$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(ma2_acf_residuals , main = "ARMA(0,1,2) ACF Residuals")
plot(ma2_pacf_residuals, main = "ARMA(0,1,2) PACF Residuals")
qqnorm(residuals(model_ma2), main = 'ARMA(0,1,2 Residuals')
qqline(model_ma2$residuals, col = 'red')

# Q-Test
ma2_box_test <- Box.test(model_ma2$residuals, lag = 3, type = "Ljung")
ma2_box_test # X-squared = 0.12569, df = 3, p-value = 0.9886 we cannot reject Null Hypothesis of no serial correlations, or residuals are independently distributed

# Forecast 6 months
fcast_ma2 = forecast(model_ma2, h=6)
autoplot(fcast_ma2, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_ma2)

```

The AIC is 685.25, much better than MA(1) and ARMA(2,2), but only marginally better than AR(2). The adjusted r-squared is 0.961485, just slighly lower than AR(2) and ARMA(2,2), and RMSE was 1.275489. The Ljung box test p-value looked good at 0.9886 at 3 lags.


# Fit Model ARMA(1,1,2)
```{r, echo=FALSE}
model_arma112 <- arima(working_hours_ts, order = c(1,1,2))
summary(model_arma112) # RMSE = 1.275173
plot(model_arma112)
coeftest(model_arma112)

# R-Squared test methodology   # https://www.statology.org/sst-ssr-sse-in-r/
# Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]
# n = Number of points
# K = # of variables excluding constant

sse <- sum((fitted(model_arma112) - working_hours_diff)^2) # 2141317 - Note using difference working hours
ssr <- sum((fitted(model_arma112) - mean(working_hours_diff))^2) # 2150359
sst <- ssr + sse # 4291676

n = length(working_hours_diff) # 204
R2_model_arma112 <- (1 - (ssr/sst)) # 0.4989465
AR2_model_arma112 <- 1-((1-R2_model_arma112)*(n-1)/(n-3-1))  # 0.4914307

# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_ts) # 205
arma112_R2 = cor(fitted(model_arma112), working_hours_ts)^2  # 0.9618873
arma112_AR2 = 1 - (1-arma112_R2)*(n-1)/(n-2-1) # 0.96151

AIC(model_arma112) # 687.1508
BIC(model_arma112) # 695.2043 

# Residuals check ACF/PACF
checkresiduals(model_arma112)
density_arma112 <- density(model_arma112$residuals)
plot(density_arma112, xlim = c(-5,5), main = 'ARMA(1,1,2 Residuals Density') 

arma112_acf_residuals <- acf(model_arma112$residuals, main = "ACf Residuals")
arma112_pacf_residuals <- acf(model_arma112$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(arma112_acf_residuals , main = "ARMA(1,1,2) ACF Residuals")
plot(arma112_pacf_residuals, main = "ARMA(1,1,2) PACF Residuals")
qqnorm(residuals(model_arma112), main = 'ARMA(1,1,2 Residuals')
qqline(model_arma112$residuals, col = 'red')

# Q-Test
arma112_box_test <- Box.test(model_arma112$residuals, lag = 4, type = "Ljung")
arma112_box_test #X-squared = 0.065107, df = 4, p-value = 0.9995 we cannot reject Null Hypothesis of no serial correlations, or residuals are independently distributed

# Forecast 6 months
fcast_arma112 = forecast(model_arma112, h=6)
autoplot(fcast_arma112, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_arma112) 
```

Adjusted r-squared was 0.9605079.


# Fit Model ARMA(2,1,1)  
```{r, echo=FALSE}
model_arma211 <- arima(working_hours_ts, order = c(2,1,1), include.mean = TRUE)
summary(model_arma211) # RMSE = 1.275937
coeftest(model_arma211)
plot(model_arma211)

sse <- sum((fitted(model_arma211) - working_hours_diff)^2) # 2141424 - Note using difference working hours
ssr <- sum((fitted(model_arma211) - mean(working_hours_diff))^2) # 2150466
sst <- ssr + sse # 4291890

n = length(working_hours_diff)
R2_model_arma211 <- (1 - (ssr/sst)) # 0.4989468
AR2_model_arma211 <- 1-((1-R2_model_arma211)*(n-1)/(n-3-1))  #   0.4914682  

# R-Squared from Week 6-Chapter 8 practice problem set
n = length(working_hours_diff) # 205
arma211_R2 = cor(fitted(model_arma211), working_hours_ts)^2  # 0.9618366
arma211_AR2 = 1 - (1-arma211_R2)*(n-1)/(n-2-1) # 0.9614568

AIC(model_arma211) # 687.39
BIC(model_arma211) # 700.6634

# Residuals check ACF/PACF
checkresiduals(model_arma211)
density_arma211 <- density(model_arma211$residuals)
plot(density_arma211, xlim = c(-5,5), main = 'ARMA(2,1,1 Residuals Density') 

arma211_acf_residuals <- acf(model_arma211$residuals, main = "ACF Residuals")
arma211_pacf_residuals <- acf(model_arma211$residuals, main = "PACF Residuals")

par(mfrow = c(3,1), mex = 0.6, cex = 0.8)
plot(arma211_acf_residuals, main = "ARMA(2,1,1) ACF Residuals")
plot(arma211_pacf_residuals, main = "ARMA(2,1,1) PACF Residuals")
qqnorm(residuals(model_arma211), main = 'ARMA(2,1,1) Residuals')
qqline(model_arma211$residuals, col = 'red')

# Q-Test
arma211_box_test <- Box.test(model_arma211$residuals, lag = 4, type = "Ljung")
arma211_box_test #X-squared = 0.29697, df = 4, p-value = 0.99 we cannot reject Null Hypothesis of no serial correlations, or residuals are independently distributed

# Forecast 6 months
fcast_arma211 = forecast(model_arma211, h=6)
autoplot(fcast_arma211, include = 12, ylab = "Working hours - Base 2007 with 100") 
summary(fcast_arma211)
```
# Step 2 DTC
## Forecasting Environment for Train/Test split: Estimation Sample and Prediciton Sample.
There are 204 observations and we'll be doing a 90%/10% split, thus our estimation sample with have 183 observations and our prediction sample will have 21 observations. We'll be applying a fixed scheme that will produce only one estimate and does not allow for parament updating. 

L(e) = ae^2 or Quadratic loss function.  h= 1

```{r, echo = FALSE}
es_work <- window(working_hours_diff,start=c(2006,4),end=c(2021,6)) 
length(es_work)# 183 observations
ps_work <- window(working_hours_diff, start=c(2021,7),end=c(2023,3))
length(ps_work) #21

# ARMA(2,1,2)
model_arma2112_estimate <- arima(es_work, order = c(2,1,2))
fcast1 <-forecast(model_arma2112_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                    # -0.05683105 -1.775835 1.662173 -2.685821 2.572158
e1 <- ps_work - fcast1$mean # 0.7568311
MSE1 <- mean(e1^2) # 0.5727932

# ARMA(2,1,0)
model_arma210_estimate <- arima(es_work, order = c(2,1,0))
fcast2 <-forecast(model_arma210_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                    # 0.2416706    -1.821441 2.304783 -2.913587 3.39692   
e2 <- ps_work - fcast2$mean # 0.4583294
MSE2 <- mean(e2^2) # 0.2100658

# ARMA(0,1,1)
model_arma011_estimate <- arima(es_work, order = c(0,1,1))
fcast3 <-forecast(model_arma011_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                  # 0.05519126     -1.69093 1.801313 -2.615271 2.725654  
e3 <- ps_work - fcast3$mean # 0.6448087
MSE3 <- mean(e3^2) # 0.4157783

# ARMA(0,1,2)
model_arma012_estimate <- arima(es_work, order = c(0,1,2))
fcast4 <-forecast(model_arma012_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                  # 0.05086249     -1.694539 1.796264 -2.618499 2.720224 
e4 <- ps_work - fcast4$mean # 0.6491375
MSE4 <- mean(e4^2) # 0.4213795

# ARMA(1,1,2)
model_arma112_estimate <- arima(es_work, order = c(1,1,2))
fcast5 <-forecast(model_arma112_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                  # -0.07516873    -1.804234 1.653896 -2.719546 2.569208 
e5 <- ps_work - fcast5$mean # 0.7751687
MSE5 <- mean(e5^2) # 0.6008866

# ARMA(2,1,1)
model_arma211_estimate <- arima(es_work, order = c(1,1,2))
fcast6 <-forecast(model_arma211_estimate, h= 1)  # Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95
                                                  # -0.07516873    -1.804234 1.653896 -2.719546 2.569208
e6 <- ps_work - fcast6$mean # 0.7751687
MSE6 <- mean(e6^2) # 0.6008866


###########################################################################################################################


# Fixed Scheme
fcast1_me<-numeric(21) #generate a vector of 21 zeros, using 183 as estimation sample and 21 as prediction sample.
fixed_fcast_ar3 <- dynlm(working_hours_diff ~ stats::lag(working_hours_diff,-1) + stats::lag(working_hours_diff,-3),
                    start = c(2006,4), end = c(2021,06))
for (i in 1:21){ #start a for loop
  fixed_fcast_ar3[i]<-coef(fixed_fcast_ar3)[1]+coef(fixed_fcast_ar3)[2]*working_hours_diff[182+i]
                        +coef(fixed_fcast_ar3)[3]*working_hours_diff[179+i]    
}                   

############ Consider emulating dynlm with different types

# AR2 process
fcast2_me<-numeric(21) #generate a vector of 21 zeros, using 183 as estimation sample and 21 as prediction sample.
fixed_fcast_ar2 <- dynlm(working_hours_diff ~ stats::lag(working_hours_diff,-1) + stats::lag(working_hours_diff,-2),
                    start = c(2006,4), end = c(2021,06))
for (i in 1:21){ #start a for loop
  fixed_fcast_ar2[i]<-coef(fixed_fcast_ar2)[1]+coef(fixed_fcast_ar2)[2]*working_hours_diff[182+i]
                        +coef(fixed_fcast_ar2)[3]*working_hours_diff[179+i]    
}  

aic_fixed_ar3 <- AIC(fixed_fcast_ar3) # 630.799
bic_fixed_ar3 <- BIC(fixed_fcast_ar3) # 643.5709

# TODO: FIX 
#checkresiduals(fixed_fcast_ar3)

# TODO: Fix All of this. G is instantiated out of order and prices is not defined prior to g.
# model<-dynlm(g ~ stats::lag(g,-1)+ stats::lag(g,-3), start=c(1975,1), end=c(2002,4)) #fit AR(3), last 20 in t.s. stops
# for (i in 1:20){ #start a for loop
#  fcast1[i]<-coef(model)[1]+coef(model)[2]*g[111+i]+coef(model)[3]*g[109+i] #fill in forecasted values at the end of each iteration }
#end the loop. We have created a numeric vector of last 20 observations (empty). we use 112 observations
# in estimation sample to predict last 20 observations in our ts. 

# Turning Prices into time series
# g <- ts(prices, frequency = 4, start=c(1975,1))

# es_prices <- window(prices_ts,start=c(1975,1),end=c(2002,4)) 
# ps_prices <- window(prices_ts,start=c(2002,5),end=c(2007,4))

# model_ar2 <- arima(es_prices, order= c(3,0,0))



```




--------------------------------------------------------------------------------
Fit on ARIMA model- this is for later as we go futher into seasonal ARIMA models.
d= 1 takes first diff of orig. data behind scenes. D=1 takes out first seasonal difference.

```{r, echo=TRUE}
fit_arima <- auto.arima(working_hours_ts, d = 1, D=1, stepwise = FALSE, 
                        approximation = FALSE, trace = TRUE) # SD = sqrt(1.786) = 1.336413 Aggregated Monthly Hours
print(summary(fit_arima)) # best model = additive, None, None (simple exponential smoothing with additive errors)
checkresiduals(fit_arima)
plot(fit_arima) # the values fit within circle,but looks funky and maybe need to omit?  ---- Yes, we only need 3 models from what I can tell. 
```

# 6 month forecast with auto.ARIMA model
```{r, echo=FALSE}
fcast2 = forecast(fit_arima, h =6)

autoplot(fcast2, include = 12)
print(summary(fcast2))
```

-------------------------------------------------------------------------------
# Down the rabbit hole
## Seasonal Naive Method - exploratory only - still have ACF spikes (associated with Great Recession + Covid)
```{r, echo=FALSE}
fit <- snaive(working_hours_diff) # Residual SD = 1.8805 Aggregated Monthly Hours
print(summary(fit))
checkresiduals(fit)
ggAcf(working_hours_diff, main ="Autocorrelation Function (ACF)", xlab ="Lag", ylab ="ACF")
ggPacf(working_hours_diff, main ="Partial Autocorrelation Function (PACF)", xlab ="Lag", ylab ="PACF")
```

## Stacking graphs/ MA(1) and arima- how to
```{r}
# y = arima(working_hours_diff, order = c(0,0,1))
# summary(y)
# 
# par(mar = c(1,1,1,1))
# 
# layout(matrix(c(1,1,2,2,), 2,2, byrow = TRUE))
# layout(matrix(c(1,1,2,2,), 2,2, byrow = TRUE))
# plot.ts(df)
# plot.ts(df)
```

# AUTO-ARIMA TIME
```{r, echo = FALSE}
fit_arima <- auto.arima(working_hours_ts, d = 1, D=1, 
                        stepwise = FALSE, approximation = FALSE, trace = TRUE)
```

